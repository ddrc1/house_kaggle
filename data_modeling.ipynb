{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb535829-ce7c-428a-baa2-6b3383c15b90",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9c726da-6f8d-4195-9f44-f421d5dcf846",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql import functions as func\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "#from spark_sklearn import GridSearchCV\n",
    "import pandas\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28d8d760-9964-4a7f-b3bf-daa28bce6b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/16 16:38:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://daniel-Nitro-AN515-51:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://192.168.1.85:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>House_analysis</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f32ad00cb50>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.master('spark://192.168.1.85:7077').appName('House_analysis').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fa9235-3f08-459b-b10e-07f76061a26a",
   "metadata": {},
   "source": [
    "## Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75cdd01e-e28a-4779-b737-5faaf6af1412",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/16 16:38:58 WARN TaskSetManager: Lost task 0.0 in stage 1.0 (TID 1) (172.29.174.150 executor 0): java.io.FileNotFoundException: \n",
      "File file:/mnt/Arquivos Trabalho/Projetos/Python/competição-kaggle1/data/house_data/part-00000-9046f244-45ba-444e-8ed5-d8bdfd262ec5-c000.csv does not exist\n",
      "\n",
      "It is possible the underlying files have been updated. You can explicitly invalidate\n",
      "the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by\n",
      "recreating the Dataset/DataFrame involved.\n",
      "       \n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:648)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:212)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:270)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)\n",
      "\tat scala.collection.Iterator$$anon$6.hasNext(Iterator.scala:470)\n",
      "\tat scala.collection.Iterator$$anon$6.hasNext(Iterator.scala:470)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)\n",
      "\tat scala.collection.IterableOnceOps.foldLeft(IterableOnce.scala:645)\n",
      "\tat scala.collection.IterableOnceOps.foldLeft$(IterableOnce.scala:642)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1293)\n",
      "\tat scala.collection.IterableOnceOps.aggregate(IterableOnce.scala:1086)\n",
      "\tat scala.collection.IterableOnceOps.aggregate$(IterableOnce.scala:1086)\n",
      "\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1293)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1198)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2322)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/16 16:38:59 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/16 16:38:59 WARN TaskSetManager: Lost task 0.0 in stage 2.0 (TID 4) (172.29.174.150 executor 0): java.io.FileNotFoundException: \n",
      "File file:/mnt/Arquivos Trabalho/Projetos/Python/competição-kaggle1/data/house_data/part-00000-9046f244-45ba-444e-8ed5-d8bdfd262ec5-c000.csv does not exist\n",
      "\n",
      "It is possible the underlying files have been updated. You can explicitly invalidate\n",
      "the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by\n",
      "recreating the Dataset/DataFrame involved.\n",
      "       \n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:648)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:212)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:270)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "23/05/16 16:39:00 ERROR TaskSetManager: Task 0 in stage 2.0 failed 4 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o34.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 8) (172.29.174.150 executor 0): java.io.FileNotFoundException: \nFile file:/mnt/Arquivos Trabalho/Projetos/Python/competição-kaggle1/data/house_data/part-00000-9046f244-45ba-444e-8ed5-d8bdfd262ec5-c000.csv does not exist\n\nIt is possible the underlying files have been updated. You can explicitly invalidate\nthe cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by\nrecreating the Dataset/DataFrame involved.\n       \n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:648)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:212)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:270)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.immutable.List.foreach(List.scala:333)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:437)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:424)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3688)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3685)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.io.FileNotFoundException: \nFile file:/mnt/Arquivos Trabalho/Projetos/Python/competição-kaggle1/data/house_data/part-00000-9046f244-45ba-444e-8ed5-d8bdfd262ec5-c000.csv does not exist\n\nIt is possible the underlying files have been updated. You can explicitly invalidate\nthe cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by\nrecreating the Dataset/DataFrame involved.\n       \n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:648)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:212)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:270)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mcsv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/house_data\u001b[39m\u001b[38;5;124m'\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, inferSchema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/kaggle/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:205\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;66;03m# Below is toPandas without Arrow optimization.\u001b[39;00m\n\u001b[0;32m--> 205\u001b[0m pdf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_records(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m    206\u001b[0m column_counter \u001b[38;5;241m=\u001b[39m Counter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m    208\u001b[0m corrected_dtypes: List[Optional[Type]] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema)\n",
      "File \u001b[0;32m~/miniconda3/envs/kaggle/lib/python3.10/site-packages/pyspark/sql/dataframe.py:817\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    807\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[1;32m    808\u001b[0m \n\u001b[1;32m    809\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;124;03m[Row(age=2, name='Alice'), Row(age=5, name='Bob')]\u001b[39;00m\n\u001b[1;32m    815\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc):\n\u001b[0;32m--> 817\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[0;32m~/miniconda3/envs/kaggle/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/miniconda3/envs/kaggle/lib/python3.10/site-packages/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/miniconda3/envs/kaggle/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o34.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 8) (172.29.174.150 executor 0): java.io.FileNotFoundException: \nFile file:/mnt/Arquivos Trabalho/Projetos/Python/competição-kaggle1/data/house_data/part-00000-9046f244-45ba-444e-8ed5-d8bdfd262ec5-c000.csv does not exist\n\nIt is possible the underlying files have been updated. You can explicitly invalidate\nthe cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by\nrecreating the Dataset/DataFrame involved.\n       \n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:648)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:212)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:270)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.immutable.List.foreach(List.scala:333)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:437)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:424)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3688)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3685)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.io.FileNotFoundException: \nFile file:/mnt/Arquivos Trabalho/Projetos/Python/competição-kaggle1/data/house_data/part-00000-9046f244-45ba-444e-8ed5-d8bdfd262ec5-c000.csv does not exist\n\nIt is possible the underlying files have been updated. You can explicitly invalidate\nthe cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by\nrecreating the Dataset/DataFrame involved.\n       \n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:648)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:212)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:270)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('./data/house_data', header=True, inferSchema=True)\n",
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5b60a4a-f27e-4953-b936-915fbab1e4ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                  (0 + 1) / 1][Stage 7:=========>         (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/16 16:37:29 WARN TaskSetManager: Lost task 1.0 in stage 7.0 (TID 22) (172.29.174.150 executor 0): java.io.FileNotFoundException: \n",
      "File file:/mnt/Arquivos Trabalho/Projetos/Python/competição-kaggle1/data/house_data/part-00001-9046f244-45ba-444e-8ed5-d8bdfd262ec5-c000.csv does not exist\n",
      "\n",
      "It is possible the underlying files have been updated. You can explicitly invalidate\n",
      "the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by\n",
      "recreating the Dataset/DataFrame involved.\n",
      "       \n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:648)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:212)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:270)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DwellingType</th>\n",
       "      <th>ZoneType</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>Flatness</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>LotConfig</th>\n",
       "      <th>...</th>\n",
       "      <th>NonbuiltArea</th>\n",
       "      <th>BuiltAreaRatio</th>\n",
       "      <th>IsDuplex</th>\n",
       "      <th>SoldAfterRemodel</th>\n",
       "      <th>BathroomsBedroomsRatio</th>\n",
       "      <th>PercentFrontage</th>\n",
       "      <th>ImpactView</th>\n",
       "      <th>RoomsMeanSize</th>\n",
       "      <th>RegularRooms</th>\n",
       "      <th>GarageAreaPerCars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2-STORY 1946 &amp; NEWER</td>\n",
       "      <td>Residential Low Density</td>\n",
       "      <td>65</td>\n",
       "      <td>8450</td>\n",
       "      <td>Paved</td>\n",
       "      <td>Not Alley Access</td>\n",
       "      <td>Regular</td>\n",
       "      <td>Near Flat/Level</td>\n",
       "      <td>All public Utilities</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>6740</td>\n",
       "      <td>0.202367</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.007692</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>213.750000</td>\n",
       "      <td>4</td>\n",
       "      <td>274.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1-STORY 1946 &amp; NEWER ALL STYLES</td>\n",
       "      <td>Residential Low Density</td>\n",
       "      <td>80</td>\n",
       "      <td>9600</td>\n",
       "      <td>Paved</td>\n",
       "      <td>Not Alley Access</td>\n",
       "      <td>Regular</td>\n",
       "      <td>Near Flat/Level</td>\n",
       "      <td>All public Utilities</td>\n",
       "      <td>FR2</td>\n",
       "      <td>...</td>\n",
       "      <td>8338</td>\n",
       "      <td>0.131458</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.008333</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>210.333333</td>\n",
       "      <td>2</td>\n",
       "      <td>230.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2-STORY 1946 &amp; NEWER</td>\n",
       "      <td>Residential Low Density</td>\n",
       "      <td>68</td>\n",
       "      <td>11250</td>\n",
       "      <td>Paved</td>\n",
       "      <td>Not Alley Access</td>\n",
       "      <td>Slightly irregular</td>\n",
       "      <td>Near Flat/Level</td>\n",
       "      <td>All public Utilities</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>9464</td>\n",
       "      <td>0.158756</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.006044</td>\n",
       "      <td>0.030222</td>\n",
       "      <td>297.666667</td>\n",
       "      <td>2</td>\n",
       "      <td>304.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2-STORY 1945 &amp; OLDER</td>\n",
       "      <td>Residential Low Density</td>\n",
       "      <td>60</td>\n",
       "      <td>9550</td>\n",
       "      <td>Paved</td>\n",
       "      <td>Not Alley Access</td>\n",
       "      <td>Slightly irregular</td>\n",
       "      <td>Near Flat/Level</td>\n",
       "      <td>All public Utilities</td>\n",
       "      <td>Corner</td>\n",
       "      <td>...</td>\n",
       "      <td>7833</td>\n",
       "      <td>0.179791</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.006283</td>\n",
       "      <td>0.031414</td>\n",
       "      <td>245.285714</td>\n",
       "      <td>3</td>\n",
       "      <td>214.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2-STORY 1946 &amp; NEWER</td>\n",
       "      <td>Residential Low Density</td>\n",
       "      <td>84</td>\n",
       "      <td>14260</td>\n",
       "      <td>Paved</td>\n",
       "      <td>Not Alley Access</td>\n",
       "      <td>Slightly irregular</td>\n",
       "      <td>Near Flat/Level</td>\n",
       "      <td>All public Utilities</td>\n",
       "      <td>FR2</td>\n",
       "      <td>...</td>\n",
       "      <td>12062</td>\n",
       "      <td>0.154137</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>0.005891</td>\n",
       "      <td>0.029453</td>\n",
       "      <td>244.222222</td>\n",
       "      <td>4</td>\n",
       "      <td>278.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>2-STORY 1946 &amp; NEWER</td>\n",
       "      <td>Residential Low Density</td>\n",
       "      <td>62</td>\n",
       "      <td>7917</td>\n",
       "      <td>Paved</td>\n",
       "      <td>Not Alley Access</td>\n",
       "      <td>Regular</td>\n",
       "      <td>Near Flat/Level</td>\n",
       "      <td>All public Utilities</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>6270</td>\n",
       "      <td>0.208033</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.007831</td>\n",
       "      <td>0.039156</td>\n",
       "      <td>235.285714</td>\n",
       "      <td>3</td>\n",
       "      <td>230.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>1-STORY 1946 &amp; NEWER ALL STYLES</td>\n",
       "      <td>Residential Low Density</td>\n",
       "      <td>85</td>\n",
       "      <td>13175</td>\n",
       "      <td>Paved</td>\n",
       "      <td>Not Alley Access</td>\n",
       "      <td>Regular</td>\n",
       "      <td>Near Flat/Level</td>\n",
       "      <td>All public Utilities</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>11102</td>\n",
       "      <td>0.157343</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>0.038710</td>\n",
       "      <td>296.142857</td>\n",
       "      <td>3</td>\n",
       "      <td>250.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>2-STORY 1945 &amp; OLDER</td>\n",
       "      <td>Residential Low Density</td>\n",
       "      <td>66</td>\n",
       "      <td>9042</td>\n",
       "      <td>Paved</td>\n",
       "      <td>Not Alley Access</td>\n",
       "      <td>Regular</td>\n",
       "      <td>Near Flat/Level</td>\n",
       "      <td>All public Utilities</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>6702</td>\n",
       "      <td>0.258792</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.007299</td>\n",
       "      <td>0.065693</td>\n",
       "      <td>260.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>252.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>1-STORY 1946 &amp; NEWER ALL STYLES</td>\n",
       "      <td>Residential Low Density</td>\n",
       "      <td>68</td>\n",
       "      <td>9717</td>\n",
       "      <td>Paved</td>\n",
       "      <td>Not Alley Access</td>\n",
       "      <td>Regular</td>\n",
       "      <td>Near Flat/Level</td>\n",
       "      <td>All public Utilities</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>8639</td>\n",
       "      <td>0.110940</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.006998</td>\n",
       "      <td>0.041988</td>\n",
       "      <td>215.600000</td>\n",
       "      <td>2</td>\n",
       "      <td>240.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>1-STORY 1946 &amp; NEWER ALL STYLES</td>\n",
       "      <td>Residential Low Density</td>\n",
       "      <td>75</td>\n",
       "      <td>9937</td>\n",
       "      <td>Paved</td>\n",
       "      <td>Not Alley Access</td>\n",
       "      <td>Regular</td>\n",
       "      <td>Near Flat/Level</td>\n",
       "      <td>All public Utilities</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>8681</td>\n",
       "      <td>0.126396</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.007548</td>\n",
       "      <td>0.045285</td>\n",
       "      <td>209.333333</td>\n",
       "      <td>2</td>\n",
       "      <td>276.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1460 rows × 90 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         DwellingType                 ZoneType  LotFrontage  \\\n",
       "0                2-STORY 1946 & NEWER  Residential Low Density           65   \n",
       "1     1-STORY 1946 & NEWER ALL STYLES  Residential Low Density           80   \n",
       "2                2-STORY 1946 & NEWER  Residential Low Density           68   \n",
       "3                2-STORY 1945 & OLDER  Residential Low Density           60   \n",
       "4                2-STORY 1946 & NEWER  Residential Low Density           84   \n",
       "...                               ...                      ...          ...   \n",
       "1455             2-STORY 1946 & NEWER  Residential Low Density           62   \n",
       "1456  1-STORY 1946 & NEWER ALL STYLES  Residential Low Density           85   \n",
       "1457             2-STORY 1945 & OLDER  Residential Low Density           66   \n",
       "1458  1-STORY 1946 & NEWER ALL STYLES  Residential Low Density           68   \n",
       "1459  1-STORY 1946 & NEWER ALL STYLES  Residential Low Density           75   \n",
       "\n",
       "      LotArea Street             Alley            LotShape         Flatness  \\\n",
       "0        8450  Paved  Not Alley Access             Regular  Near Flat/Level   \n",
       "1        9600  Paved  Not Alley Access             Regular  Near Flat/Level   \n",
       "2       11250  Paved  Not Alley Access  Slightly irregular  Near Flat/Level   \n",
       "3        9550  Paved  Not Alley Access  Slightly irregular  Near Flat/Level   \n",
       "4       14260  Paved  Not Alley Access  Slightly irregular  Near Flat/Level   \n",
       "...       ...    ...               ...                 ...              ...   \n",
       "1455     7917  Paved  Not Alley Access             Regular  Near Flat/Level   \n",
       "1456    13175  Paved  Not Alley Access             Regular  Near Flat/Level   \n",
       "1457     9042  Paved  Not Alley Access             Regular  Near Flat/Level   \n",
       "1458     9717  Paved  Not Alley Access             Regular  Near Flat/Level   \n",
       "1459     9937  Paved  Not Alley Access             Regular  Near Flat/Level   \n",
       "\n",
       "                 Utilities LotConfig  ... NonbuiltArea BuiltAreaRatio  \\\n",
       "0     All public Utilities    Inside  ...         6740       0.202367   \n",
       "1     All public Utilities       FR2  ...         8338       0.131458   \n",
       "2     All public Utilities    Inside  ...         9464       0.158756   \n",
       "3     All public Utilities    Corner  ...         7833       0.179791   \n",
       "4     All public Utilities       FR2  ...        12062       0.154137   \n",
       "...                    ...       ...  ...          ...            ...   \n",
       "1455  All public Utilities    Inside  ...         6270       0.208033   \n",
       "1456  All public Utilities    Inside  ...        11102       0.157343   \n",
       "1457  All public Utilities    Inside  ...         6702       0.258792   \n",
       "1458  All public Utilities    Inside  ...         8639       0.110940   \n",
       "1459  All public Utilities    Inside  ...         8681       0.126396   \n",
       "\n",
       "     IsDuplex SoldAfterRemodel BathroomsBedroomsRatio PercentFrontage  \\\n",
       "0           1                0               1.000000        0.007692   \n",
       "1           0                0               1.500000        0.008333   \n",
       "2           1                1               1.000000        0.006044   \n",
       "3           1                1               3.000000        0.006283   \n",
       "4           1                0               1.333333        0.005891   \n",
       "...       ...              ...                    ...             ...   \n",
       "1455        1                1               1.000000        0.007831   \n",
       "1456        0                1               1.500000        0.006452   \n",
       "1457        1                1               2.000000        0.007299   \n",
       "1458        0                1               2.000000        0.006998   \n",
       "1459        0                0               1.500000        0.007548   \n",
       "\n",
       "      ImpactView  RoomsMeanSize  RegularRooms  GarageAreaPerCars  \n",
       "0       0.038462     213.750000             4         274.000000  \n",
       "1       0.066667     210.333333             2         230.000000  \n",
       "2       0.030222     297.666667             2         304.000000  \n",
       "3       0.031414     245.285714             3         214.000000  \n",
       "4       0.029453     244.222222             4         278.666667  \n",
       "...          ...            ...           ...                ...  \n",
       "1455    0.039156     235.285714             3         230.000000  \n",
       "1456    0.038710     296.142857             3         250.000000  \n",
       "1457    0.065693     260.000000             4         252.000000  \n",
       "1458    0.041988     215.600000             2         240.000000  \n",
       "1459    0.045285     209.333333             2         276.000000  \n",
       "\n",
       "[1460 rows x 90 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "df_eval = df[df[\"SalePrice\"].isNotNull()]\n",
    "final_test_df = df[df[\"SalePrice\"].isNull()]\n",
    "df_eval.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70b4be73-8cc9-41e5-abec-a6c8627cc64a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/16 16:37:38 WARN TaskSetManager: Lost task 1.0 in stage 8.0 (TID 26) (172.29.174.150 executor 0): java.io.FileNotFoundException: \n",
      "File file:/mnt/Arquivos Trabalho/Projetos/Python/competição-kaggle1/data/house_data/part-00001-9046f244-45ba-444e-8ed5-d8bdfd262ec5-c000.csv does not exist\n",
      "\n",
      "It is possible the underlying files have been updated. You can explicitly invalidate\n",
      "the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by\n",
      "recreating the Dataset/DataFrame involved.\n",
      "       \n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:648)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:212)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:270)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DwellingType</th>\n",
       "      <th>ZoneType</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>Flatness</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>LotConfig</th>\n",
       "      <th>...</th>\n",
       "      <th>NonbuiltArea</th>\n",
       "      <th>BuiltAreaRatio</th>\n",
       "      <th>IsDuplex</th>\n",
       "      <th>SoldAfterRemodel</th>\n",
       "      <th>BathroomsBedroomsRatio</th>\n",
       "      <th>PercentFrontage</th>\n",
       "      <th>ImpactView</th>\n",
       "      <th>RoomsMeanSize</th>\n",
       "      <th>RegularRooms</th>\n",
       "      <th>GarageAreaPerCars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1-STORY 1946 &amp; NEWER ALL STYLES</td>\n",
       "      <td>Residential High Density</td>\n",
       "      <td>80</td>\n",
       "      <td>11622</td>\n",
       "      <td>Paved</td>\n",
       "      <td>Not Alley Access</td>\n",
       "      <td>Regular</td>\n",
       "      <td>Near Flat/Level</td>\n",
       "      <td>All public Utilities</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>10726</td>\n",
       "      <td>0.077095</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.006883</td>\n",
       "      <td>0.041301</td>\n",
       "      <td>179.200000</td>\n",
       "      <td>2</td>\n",
       "      <td>730.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1-STORY 1946 &amp; NEWER ALL STYLES</td>\n",
       "      <td>Residential Low Density</td>\n",
       "      <td>81</td>\n",
       "      <td>14267</td>\n",
       "      <td>Paved</td>\n",
       "      <td>Not Alley Access</td>\n",
       "      <td>Slightly irregular</td>\n",
       "      <td>Near Flat/Level</td>\n",
       "      <td>All public Utilities</td>\n",
       "      <td>Corner</td>\n",
       "      <td>...</td>\n",
       "      <td>12938</td>\n",
       "      <td>0.093152</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.005677</td>\n",
       "      <td>0.034065</td>\n",
       "      <td>221.500000</td>\n",
       "      <td>2</td>\n",
       "      <td>312.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2-STORY 1946 &amp; NEWER</td>\n",
       "      <td>Residential Low Density</td>\n",
       "      <td>74</td>\n",
       "      <td>13830</td>\n",
       "      <td>Paved</td>\n",
       "      <td>Not Alley Access</td>\n",
       "      <td>Slightly irregular</td>\n",
       "      <td>Near Flat/Level</td>\n",
       "      <td>All public Utilities</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>12201</td>\n",
       "      <td>0.117787</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.005351</td>\n",
       "      <td>0.026753</td>\n",
       "      <td>271.500000</td>\n",
       "      <td>2</td>\n",
       "      <td>241.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2-STORY 1946 &amp; NEWER</td>\n",
       "      <td>Residential Low Density</td>\n",
       "      <td>78</td>\n",
       "      <td>9978</td>\n",
       "      <td>Paved</td>\n",
       "      <td>Not Alley Access</td>\n",
       "      <td>Slightly irregular</td>\n",
       "      <td>Near Flat/Level</td>\n",
       "      <td>All public Utilities</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>8374</td>\n",
       "      <td>0.160754</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.007817</td>\n",
       "      <td>0.046903</td>\n",
       "      <td>229.142857</td>\n",
       "      <td>3</td>\n",
       "      <td>235.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1-STORY PUD (Planned Unit Development) - 1946 ...</td>\n",
       "      <td>Residential Low Density</td>\n",
       "      <td>43</td>\n",
       "      <td>5005</td>\n",
       "      <td>Paved</td>\n",
       "      <td>Not Alley Access</td>\n",
       "      <td>Slightly irregular</td>\n",
       "      <td>Hillside</td>\n",
       "      <td>All public Utilities</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>3725</td>\n",
       "      <td>0.255744</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.008591</td>\n",
       "      <td>0.042957</td>\n",
       "      <td>256.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>253.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>2-STORY PUD - 1946 &amp; NEWER</td>\n",
       "      <td>Residential Medium Density</td>\n",
       "      <td>21</td>\n",
       "      <td>1936</td>\n",
       "      <td>Paved</td>\n",
       "      <td>Not Alley Access</td>\n",
       "      <td>Regular</td>\n",
       "      <td>Near Flat/Level</td>\n",
       "      <td>All public Utilities</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>844</td>\n",
       "      <td>0.564050</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.010847</td>\n",
       "      <td>0.075930</td>\n",
       "      <td>218.400000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>2-STORY PUD - 1946 &amp; NEWER</td>\n",
       "      <td>Residential Medium Density</td>\n",
       "      <td>21</td>\n",
       "      <td>1894</td>\n",
       "      <td>Paved</td>\n",
       "      <td>Not Alley Access</td>\n",
       "      <td>Regular</td>\n",
       "      <td>Near Flat/Level</td>\n",
       "      <td>All public Utilities</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>802</td>\n",
       "      <td>0.576558</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.011088</td>\n",
       "      <td>0.055438</td>\n",
       "      <td>182.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>286.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>1-STORY 1946 &amp; NEWER ALL STYLES</td>\n",
       "      <td>Residential Low Density</td>\n",
       "      <td>160</td>\n",
       "      <td>20000</td>\n",
       "      <td>Paved</td>\n",
       "      <td>Not Alley Access</td>\n",
       "      <td>Regular</td>\n",
       "      <td>Near Flat/Level</td>\n",
       "      <td>All public Utilities</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>18776</td>\n",
       "      <td>0.061200</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>0.056000</td>\n",
       "      <td>174.857143</td>\n",
       "      <td>2</td>\n",
       "      <td>288.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>SPLIT FOYER</td>\n",
       "      <td>Residential Low Density</td>\n",
       "      <td>62</td>\n",
       "      <td>10441</td>\n",
       "      <td>Paved</td>\n",
       "      <td>Not Alley Access</td>\n",
       "      <td>Regular</td>\n",
       "      <td>Near Flat/Level</td>\n",
       "      <td>All public Utilities</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>9471</td>\n",
       "      <td>0.092903</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.005938</td>\n",
       "      <td>0.029691</td>\n",
       "      <td>161.666667</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>2-STORY 1946 &amp; NEWER</td>\n",
       "      <td>Residential Low Density</td>\n",
       "      <td>74</td>\n",
       "      <td>9627</td>\n",
       "      <td>Paved</td>\n",
       "      <td>Not Alley Access</td>\n",
       "      <td>Regular</td>\n",
       "      <td>Near Flat/Level</td>\n",
       "      <td>All public Utilities</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>7627</td>\n",
       "      <td>0.207749</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.007687</td>\n",
       "      <td>0.038434</td>\n",
       "      <td>222.222222</td>\n",
       "      <td>5</td>\n",
       "      <td>216.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1459 rows × 90 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           DwellingType  \\\n",
       "0                       1-STORY 1946 & NEWER ALL STYLES   \n",
       "1                       1-STORY 1946 & NEWER ALL STYLES   \n",
       "2                                  2-STORY 1946 & NEWER   \n",
       "3                                  2-STORY 1946 & NEWER   \n",
       "4     1-STORY PUD (Planned Unit Development) - 1946 ...   \n",
       "...                                                 ...   \n",
       "1454                         2-STORY PUD - 1946 & NEWER   \n",
       "1455                         2-STORY PUD - 1946 & NEWER   \n",
       "1456                    1-STORY 1946 & NEWER ALL STYLES   \n",
       "1457                                        SPLIT FOYER   \n",
       "1458                               2-STORY 1946 & NEWER   \n",
       "\n",
       "                        ZoneType  LotFrontage  LotArea Street  \\\n",
       "0       Residential High Density           80    11622  Paved   \n",
       "1        Residential Low Density           81    14267  Paved   \n",
       "2        Residential Low Density           74    13830  Paved   \n",
       "3        Residential Low Density           78     9978  Paved   \n",
       "4        Residential Low Density           43     5005  Paved   \n",
       "...                          ...          ...      ...    ...   \n",
       "1454  Residential Medium Density           21     1936  Paved   \n",
       "1455  Residential Medium Density           21     1894  Paved   \n",
       "1456     Residential Low Density          160    20000  Paved   \n",
       "1457     Residential Low Density           62    10441  Paved   \n",
       "1458     Residential Low Density           74     9627  Paved   \n",
       "\n",
       "                 Alley            LotShape         Flatness  \\\n",
       "0     Not Alley Access             Regular  Near Flat/Level   \n",
       "1     Not Alley Access  Slightly irregular  Near Flat/Level   \n",
       "2     Not Alley Access  Slightly irregular  Near Flat/Level   \n",
       "3     Not Alley Access  Slightly irregular  Near Flat/Level   \n",
       "4     Not Alley Access  Slightly irregular         Hillside   \n",
       "...                ...                 ...              ...   \n",
       "1454  Not Alley Access             Regular  Near Flat/Level   \n",
       "1455  Not Alley Access             Regular  Near Flat/Level   \n",
       "1456  Not Alley Access             Regular  Near Flat/Level   \n",
       "1457  Not Alley Access             Regular  Near Flat/Level   \n",
       "1458  Not Alley Access             Regular  Near Flat/Level   \n",
       "\n",
       "                 Utilities LotConfig  ... NonbuiltArea BuiltAreaRatio  \\\n",
       "0     All public Utilities    Inside  ...        10726       0.077095   \n",
       "1     All public Utilities    Corner  ...        12938       0.093152   \n",
       "2     All public Utilities    Inside  ...        12201       0.117787   \n",
       "3     All public Utilities    Inside  ...         8374       0.160754   \n",
       "4     All public Utilities    Inside  ...         3725       0.255744   \n",
       "...                    ...       ...  ...          ...            ...   \n",
       "1454  All public Utilities    Inside  ...          844       0.564050   \n",
       "1455  All public Utilities    Inside  ...          802       0.576558   \n",
       "1456  All public Utilities    Inside  ...        18776       0.061200   \n",
       "1457  All public Utilities    Inside  ...         9471       0.092903   \n",
       "1458  All public Utilities    Inside  ...         7627       0.207749   \n",
       "\n",
       "     IsDuplex SoldAfterRemodel BathroomsBedroomsRatio PercentFrontage  \\\n",
       "0           0                0                    2.0        0.006883   \n",
       "1           0                0                    1.5        0.005677   \n",
       "2           1                1                    1.0        0.005351   \n",
       "3           1                0                    1.0        0.007817   \n",
       "4           0                0                    1.0        0.008591   \n",
       "...       ...              ...                    ...             ...   \n",
       "1454        1                0                    1.5        0.010847   \n",
       "1455        1                0                    1.5        0.011088   \n",
       "1456        0                1                    4.0        0.008000   \n",
       "1457        0                0                    3.0        0.005938   \n",
       "1458        1                1                    1.0        0.007687   \n",
       "\n",
       "      ImpactView  RoomsMeanSize  RegularRooms  GarageAreaPerCars  \n",
       "0       0.041301     179.200000             2         730.000000  \n",
       "1       0.034065     221.500000             2         312.000000  \n",
       "2       0.026753     271.500000             2         241.000000  \n",
       "3       0.046903     229.142857             3         235.000000  \n",
       "4       0.042957     256.000000             2         253.000000  \n",
       "...          ...            ...           ...                ...  \n",
       "1454    0.075930     218.400000             1           0.000000  \n",
       "1455    0.055438     182.000000             2         286.000000  \n",
       "1456    0.056000     174.857143             2         288.000000  \n",
       "1457    0.029691     161.666667             2           0.000000  \n",
       "1458    0.038434     222.222222             5         216.666667  \n",
       "\n",
       "[1459 rows x 90 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "final_test_df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c438421d-f33f-461a-a0d7-155eb550cb78",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b33e99b-d06d-4d7c-a75d-a15877f86368",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = [col for col, dtype in df.dtypes if dtype != 'string']\n",
    "numeric_cols.remove('SalePrice')\n",
    "string_cols = [col for col, dtype in df.dtypes if dtype == 'string']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b9fbb5a0-1dd3-456b-9cc4-8071c885c423",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_indexer_names = [x+\"Indexer\" for x in string_cols]\n",
    "col_onehot_names = [x+\"OneHot\" for x in string_cols]\n",
    "\n",
    "indexer = StringIndexer(inputCols=string_cols, outputCols=col_indexer_names)\n",
    "encoder = OneHotEncoder(inputCols=col_indexer_names, outputCols=col_onehot_names)\n",
    "assembler = VectorAssembler(inputCols=numeric_cols+col_onehot_names, outputCol='features')\n",
    "pipeline = Pipeline(stages=[indexer, encoder, assembler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3ed686ff-d916-4071-a29a-d036ce152088",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_model = pipeline.fit(df)\n",
    "df_eval = pipe_model.transform(df_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6ed468-9e60-4c45-8cf7-e4183df90cd4",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a5650663-297e-480c-b7e6-d9e17dcfadf9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(labelCol=\"SalePrice\", featuresCol=\"features\", )\n",
    "\n",
    "grid = ParamGridBuilder()\n",
    "grid = grid.addGrid(rf.numTrees, np.arange(100, 501, 100))\n",
    "grid = grid.addGrid(rf.featureSubsetStrategy, ['sqrt', 'log2', 1, 0.75, 0.5, 0.25])\n",
    "grid = grid.addGrid(rf.bootstrap, [True, False])\n",
    "grid = grid.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4cdf917a-2b80-404f-88bc-06aeb2a8652a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/28 20:28:46 WARN DAGScheduler: Broadcasting large task binary with size 1268.1 KiB\n",
      "23/04/28 20:28:47 WARN DAGScheduler: Broadcasting large task binary with size 1290.9 KiB\n",
      "23/04/28 20:28:49 WARN DAGScheduler: Broadcasting large task binary with size 1293.8 KiB\n",
      "23/04/28 20:28:49 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "23/04/28 20:28:51 WARN DAGScheduler: Broadcasting large task binary with size 1296.6 KiB\n",
      "23/04/28 20:28:51 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "23/04/28 20:28:52 WARN DAGScheduler: Broadcasting large task binary with size 1035.2 KiB\n",
      "23/04/28 20:28:53 WARN DAGScheduler: Broadcasting large task binary with size 1757.7 KiB\n",
      "23/04/28 20:28:54 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "23/04/28 20:28:55 WARN DAGScheduler: Broadcasting large task binary with size 1034.6 KiB\n",
      "23/04/28 20:28:56 WARN DAGScheduler: Broadcasting large task binary with size 1769.8 KiB\n",
      "23/04/28 20:28:56 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "23/04/28 20:28:59 WARN DAGScheduler: Broadcasting large task binary with size 1256.2 KiB\n",
      "23/04/28 20:29:00 WARN DAGScheduler: Broadcasting large task binary with size 1280.3 KiB\n",
      "23/04/28 20:29:01 WARN DAGScheduler: Broadcasting large task binary with size 1289.0 KiB\n",
      "23/04/28 20:29:02 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "23/04/28 20:29:03 WARN DAGScheduler: Broadcasting large task binary with size 1297.4 KiB\n",
      "23/04/28 20:29:04 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "23/04/28 20:29:05 WARN DAGScheduler: Broadcasting large task binary with size 1033.5 KiB\n",
      "23/04/28 20:29:06 WARN DAGScheduler: Broadcasting large task binary with size 1751.4 KiB\n",
      "23/04/28 20:29:06 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "23/04/28 20:29:08 WARN DAGScheduler: Broadcasting large task binary with size 1034.6 KiB\n",
      "23/04/28 20:29:08 WARN DAGScheduler: Broadcasting large task binary with size 1770.3 KiB\n",
      "23/04/28 20:29:09 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "23/04/28 20:29:12 WARN DAGScheduler: Broadcasting large task binary with size 1260.5 KiB\n",
      "23/04/28 20:29:13 WARN DAGScheduler: Broadcasting large task binary with size 1269.8 KiB\n",
      "23/04/28 20:29:14 WARN DAGScheduler: Broadcasting large task binary with size 1282.2 KiB\n",
      "23/04/28 20:29:14 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "23/04/28 20:29:16 WARN DAGScheduler: Broadcasting large task binary with size 1282.8 KiB\n",
      "23/04/28 20:29:17 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "23/04/28 20:29:18 WARN DAGScheduler: Broadcasting large task binary with size 1031.4 KiB\n",
      "23/04/28 20:29:18 WARN DAGScheduler: Broadcasting large task binary with size 1742.9 KiB\n",
      "23/04/28 20:29:19 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "23/04/28 20:29:21 WARN DAGScheduler: Broadcasting large task binary with size 1034.7 KiB\n",
      "23/04/28 20:29:21 WARN DAGScheduler: Broadcasting large task binary with size 1746.7 KiB\n",
      "23/04/28 20:29:22 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "23/04/28 20:29:24 WARN DAGScheduler: Broadcasting large task binary with size 1389.0 KiB\n",
      "23/04/28 20:29:25 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n"
     ]
    }
   ],
   "source": [
    "cv_model = CrossValidator(estimator=rf, estimatorParamMaps=grid, evaluator=RegressionEvaluator(labelCol=\"SalePrice\", metricName='rmse'), numFolds=10, parallelism=5)\n",
    "cv_model = cv.fit(dataset=df_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fd3e276b-8901-4e9f-aa4d-8401cce8166d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[32096.111524660286,\n",
       " 32279.2927153402,\n",
       " 31844.228776590317,\n",
       " 32147.486488311817,\n",
       " 32015.187533787903,\n",
       " 31997.422024942625]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_model.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6ebc7fc1-6275-4524-afe4-5146b3d210f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressionModel: uid=RandomForestRegressor_4345b270e34e, numTrees=200, numFeatures=254"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_model.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4eefca25-b08a-4bda-b108-ec766c339b54",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cv_model.extractParamMap().items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c0da93a2-b29f-42bc-a5e6-a34bbd412fc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1308, 152)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test = df_eval.randomSplit([0.9, 0.1])\n",
    "train.count(), test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "02c58252-f6ec-4d19-b0d5-99f328866584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/07 22:17:51 WARN DAGScheduler: Broadcasting large task binary with size 1239.4 KiB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rf_model = rf.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ad4ecc79-0d16-4357-b59f-64aafe60147b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = rf_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7b417eba-9e40-463f-ba5f-a2341c441fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: 0.8811069103377805\n",
      "MAE: 16008.067032715726\n",
      "RMSE: 22100.15958919812\n"
     ]
    }
   ],
   "source": [
    "print(\"R2:\", r2_score(np.array(results[['SalePrice']].collect()), np.array(results[['prediction']].collect())))\n",
    "print(\"MAE:\", mean_absolute_error(np.array(results[['SalePrice']].collect()), np.array(results[['prediction']].collect())))\n",
    "print(\"RMSE:\", mean_squared_error(np.array(results[['SalePrice']].collect()), np.array(results[['prediction']].collect())) ** (1/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "cad8015e-fec2-455c-af2c-ed8810a38988",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_df = pipe_model.transform(final_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a7694e57-257d-4a96-8e45-4bd2f2d68aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/07 22:17:54 WARN DAGScheduler: Broadcasting large task binary with size 1344.1 KiB\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor(numTrees=100, labelCol=\"SalePrice\", featuresCol=\"features\")\n",
    "rf_model = rf.fit(df_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a601b3f8-aa74-4124-a3c3-e1f2c06958d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_to_submit = rf_model.transform(final_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "1e8246cc-8e6d-4ba9-8bd3-750427906a76",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|         SalePrice|\n",
      "+------------------+\n",
      "|126053.91164766779|\n",
      "| 145333.5734536947|\n",
      "| 173196.3568250724|\n",
      "|181667.25960022534|\n",
      "|210986.88413332804|\n",
      "|180133.26180332105|\n",
      "|161756.41270636453|\n",
      "|168963.01883397638|\n",
      "|176788.15962884467|\n",
      "| 124727.4086450906|\n",
      "|198265.53304276185|\n",
      "|112912.30855791138|\n",
      "|114647.11298737467|\n",
      "|160103.92951713438|\n",
      "| 141137.2874513425|\n",
      "|377475.16232445254|\n",
      "|257701.73528888784|\n",
      "| 312498.1694346844|\n",
      "|289957.67746455566|\n",
      "|424606.52677407884|\n",
      "+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_to_submit = results_to_submit[['prediction']].withColumnRenamed('prediction', 'SalePrice')\n",
    "results_to_submit.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1ccf5096-9cd8-4a0b-adbe-c5b6e037738f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1461</td>\n",
       "      <td>126053.911648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1462</td>\n",
       "      <td>145333.573454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1463</td>\n",
       "      <td>173196.356825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1464</td>\n",
       "      <td>181667.259600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1465</td>\n",
       "      <td>210986.884133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>2915</td>\n",
       "      <td>100692.886521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>2916</td>\n",
       "      <td>107950.505939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>2917</td>\n",
       "      <td>151569.452208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>2918</td>\n",
       "      <td>120829.640975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>2919</td>\n",
       "      <td>234580.738581</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1459 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id      SalePrice\n",
       "0     1461  126053.911648\n",
       "1     1462  145333.573454\n",
       "2     1463  173196.356825\n",
       "3     1464  181667.259600\n",
       "4     1465  210986.884133\n",
       "...    ...            ...\n",
       "1454  2915  100692.886521\n",
       "1455  2916  107950.505939\n",
       "1456  2917  151569.452208\n",
       "1457  2918  120829.640975\n",
       "1458  2919  234580.738581\n",
       "\n",
       "[1459 rows x 2 columns]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_to_submit = results_to_submit.toPandas()\n",
    "results_to_submit[\"Id\"] = range(1461, 1461+len(results_to_submit))\n",
    "results_to_submit = results_to_submit[[\"Id\", \"SalePrice\"]]\n",
    "results_to_submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "f16406b6-e83e-426f-a6ff-35e9232aba29",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_to_submit.to_csv('data/submition_v3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b87a25-3ce1-4794-b745-e44da925d4e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
